{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate an RDF database\n",
    "\n",
    "This notebook reports the main steps to download CSV files, process them and create an RDF dataset from them accordingly to an ontology. \n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import unicodedata\n",
    "import hashlib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.parent.absolute())\n",
    "print(path)\n",
    "grammyUrl = path + '/csv/the_grammy_awards_mapped_uppercase.csv'\n",
    "print(grammyUrl)\n",
    "albumsUrl = path + '/csv/musicoset_metadata/albums.csv'\n",
    "print(albumsUrl)\n",
    "songsUrl = path + '/csv/musicoset_metadata/albums.csv'\n",
    "print(songsUrl)\n",
    "artistsUrl = path + '/csv/musicoset_metadata/albums.csv'\n",
    "print(artistsUrl)\n",
    "tracksUrl = path + '/csv/musicoset_metadata/tracks.csv'\n",
    "print(tracksUrl)\n",
    "songInChartUrl = path + '/csv/musicoset_popularity/song_chart.csv'\n",
    "print(songInChartUrl)\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/PopulateRDFdb/PopulateGrammyCategories/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammy & candidates/winners (Songs, Artists, Albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "grammy = pd.read_csv(grammyUrl, sep=',', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album = pd.read_csv(albumsUrl, sep='\\t', index_col='album_id', keep_default_na=False, na_values=['_'])\n",
    "album.info()\n",
    "# List used to save the triple GrammyID, album and isWinner.\n",
    "# It contains the specific grammy and the corresponding winner/candidated album\n",
    "matched_pairs_grammy_album = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install <code>RDFLib</code>\n",
    "\n",
    "<code>pip3 install rdflib </code> [Documentation](https://rdflib.readthedocs.io/en/stable/gettingstarted.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD, SKOS, RDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "ME = Namespace(\"http://www.dei.unipd.it/~gdb/ontology/melody#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mel\", ME)\n",
    "g.bind(\"skos\", SKOS)\n",
    "g.bind(\"rdfs\", RDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammy_id(year, category):\n",
    "    \n",
    "    year = str(year)\n",
    "    category = str(category) if pd.notna(category) else ''\n",
    "\n",
    "    # Data cleaning and normalization\n",
    "    def clean_text(text):\n",
    "        return re.sub(r'[^\\w\\s-]', '', text).lower().strip()\n",
    "    \n",
    "    # Create a concatenated string with all the data\n",
    "    full_string = f\"{year}_{clean_text(category)}\"\n",
    "    \n",
    "    # Generate a truncated SHA-256 hash\n",
    "    hash_object = hashlib.sha256(full_string.encode())\n",
    "    short_hash = hash_object.hexdigest()[:8]\n",
    "    \n",
    "    # Create the final ID\n",
    "    category_abbr = ''.join(word[0] for word in clean_text(category).split()[:3])\n",
    "    final_id = f\"{year}_{category_abbr}_{short_hash}\"\n",
    "    \n",
    "    return final_id\n",
    "\n",
    "\n",
    "def normalize_uri(name):\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    name = name.replace(\" \", \"-\")\n",
    "    name = name.replace(\",\", \"\").replace(\"'\", \"\")\n",
    "    name = re.sub(r'&(\\w)', lambda match: \"&\" + match.group(1).upper(), name)\n",
    "    name = name.replace(\"&\", \"n\")\n",
    "    name = re.sub(r'/(\\w)', lambda match: \"/\" + match.group(1).upper(), name)\n",
    "    name = name.replace(\"/\", \"\")\n",
    "\n",
    "    return name\n",
    "\n",
    "def string_to_bool(s):\n",
    "    if isinstance(s, bool):\n",
    "        return s\n",
    "    return s.lower() == \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Grammy individuals - Generate list containing albums that have won a grammy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "\n",
    "# Dictionary to track Grammy IDs for unique year-category combinations\n",
    "grammy_id_lookup = {}\n",
    "\n",
    "#iterate over the grammy dataframe\n",
    "for index, row in grammy.iterrows():\n",
    "\n",
    "    grammy_id = create_grammy_id(\n",
    "            row['year'],       \n",
    "            row['category']\n",
    "        )\n",
    "\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the grammy_id as URI\n",
    "    current_grammy = URIRef(ME[grammy_id])\n",
    "\n",
    "    if False:\n",
    "        print(row)\n",
    "        print(grammy_id) #1959_a_a98ae627\n",
    "        print(ME.Grammy) # http://www.dei.unipd.it/~gdb/ontology/melody/Grammy\n",
    "        print(current_grammy) # http://www.dei.unipd.it/~gdb/ontology/melody/1959_a_a98ae627\n",
    "\n",
    "    # Add Grammy, link it to SKOS category and to the corresponding dataPropery Year\n",
    "    g.add((current_grammy, RDF.type, ME.Grammy))\n",
    "    g.add((current_grammy, ME['hasCategory'], Literal(normalize_uri(row['category']))))\n",
    "    g.add((current_grammy, ME['year'], Literal(row['year'], datatype=XSD.gYear)))\n",
    "\n",
    "    # If the grammy as \"album\" in the category title\n",
    "    if \"album\" in row['category'].lower():\n",
    "\n",
    "        # Extracts the names of the artists from album['artists'].\n",
    "        # Converts string to dictionary and takes the value\n",
    "        album['artist_name'] = album['artists'].apply(lambda x: list(eval(x).values())[0]) \n",
    "         \n",
    "        isWinner = string_to_bool(row['winner'])\n",
    "\n",
    "        # Clean row['workers'] and get a list of names\n",
    "        worker_names = re.sub(r'\\([^)]*\\)', '', row['workers'])  # Remove content between brackets\n",
    "        worker_names = [name.strip() for name in worker_names.split(',')] # Split workers\n",
    "\n",
    "        # Filter albums that match by title and (at least one) artist\n",
    "        # So, matched_album contains only the albums that have won/nominated for a grammy, we resolve cases of homonymy by filtering by artist name in both grammy and artists\n",
    "        matched_album = album[\n",
    "            (album['billboard'].str.lower() == row['nominee'].lower().rstrip('.,').strip()) & \n",
    "            (album['artist_name'].str.lower().isin([name.lower() for name in worker_names]))\n",
    "        ]\n",
    "\n",
    "        if not matched_album.empty:\n",
    "            #print(matched_album)\n",
    "            for album_row in matched_album.itertuples(index=True):\n",
    "                print(grammy_id, album_row.Index, isWinner)\n",
    "                matched_pairs_grammy_album.append((grammy_id, album_row, isWinner))\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'grammy.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referential integrity\n",
    "Note that in RDF we are in an open world situation. We cannot guarantee the referential integrity between the entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Album\n",
    "\n",
    "Let us generate the RDF data relative to the movie workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums = pd.read_csv(albumsUrl, sep='\\t', index_col='album_id', keep_default_na=False, na_values=['_'])\n",
    "albums.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv(tracksUrl, sep='\\t', index_col='album_id', keep_default_na=False, na_values=['_'])\n",
    "tracks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People are modeled with the FOAF ontology. \n",
    "Refer to [FOAF Documentation](http://xmlns.com/foaf/spec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songInChart = pd.read_csv(songInChartUrl, sep='\\t', index_col='song_id', keep_default_na=False, na_values=['_'])\n",
    "songInChart.info()\n",
    "songInChart.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new graph\n",
    "g = Graph()\n",
    "ME = Namespace(\"http://www.dei.unipd.it/~gdb/ontology/melody#\")\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mel\", ME)\n",
    "g.bind(\"skos\", SKOS)\n",
    "g.bind(\"rdfs\", RDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link song to Album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the album dataframe\n",
    "for index, row in albums.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "\n",
    "    if False:\n",
    "        print(row)\n",
    "        print(index) #5n1GSzC1Reao29ScnpLYqp\n",
    "        print(ME.Album) # http://www.dei.unipd.it/~gdb/ontology/melody/Album\n",
    "        print(current_album) # http://www.dei.unipd.it/~gdb/ontology/melody/5n1GSzC1Reao29ScnpLYqp\n",
    "\n",
    "    # Rows obtained from tracks that contain all the songs of the current album (row)\n",
    "    album_tracks = tracks[tracks.index == index]\n",
    "    #print(album_tracks['song_id'])\n",
    "    #print(songInChart.index)\n",
    "    #break\n",
    "    \n",
    "    if not album_tracks.empty:\n",
    "            # Iterate through all tracks for this album\n",
    "            for _, track in album_tracks.iterrows():\n",
    "                # If the song on the album is among the top 100 songs\n",
    "                match = songInChart.index.str.contains(track['song_id']).any()\n",
    "                if match:\n",
    "                    #print(track['song_id'])\n",
    "                    song_id = track['song_id']\n",
    "                    current_song = URIRef(ME[song_id])\n",
    "                    current_album = URIRef(ME[index])\n",
    "                    g.add((current_album, RDF.type, ME.Album))\n",
    "                    g.add((current_album, ME['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "                    g.add((current_album, ME['containsSong'], current_song))\n",
    "                    if row['total_tracks'] is not None and isinstance(row['total_tracks'], int):\n",
    "                        g.add((current_album, ME['totalTracks'], Literal(row['total_tracks'], datatype=XSD.positiveInteger)))\n",
    "                    else:\n",
    "                        g.add((current_album, ME['totalTracks'], Literal(0, datatype=XSD.positiveInteger)))\n",
    "                    print(f\"\\nAlbum {index} contains song {song_id}\\nSo we add Album {row['name']} \")\n",
    "\n",
    "                    # Add link and artist\n",
    "                    name_dict = eval(row.artists)\n",
    "                    for artist_id in name_dict.keys():\n",
    "                        current_artist = URIRef(ME[artist_id])\n",
    "                        g.add((current_album, ME['isReleasedBy'], current_artist))\n",
    "                        print(f\"Artist ID: {artist_id}\")\n",
    "\n",
    "                    # Add winner/candidated to grammy winning albums\n",
    "                    for grammy_id, album_row, winner in matched_pairs_grammy_album:\n",
    "                        if album_row.Index == index:\n",
    "                            album_id = album_row.Index\n",
    "                            current_album = URIRef(ME[album_id])\n",
    "                            current_grammy = URIRef(ME[grammy_id])\n",
    "\n",
    "\n",
    "                            if (winner):\n",
    "                                g.add((current_album, ME['winner'], current_grammy))\n",
    "                                print(f\"{album_id} won {grammy_id}\")\n",
    "                            else:\n",
    "                                g.add((current_album, ME['candidated'], current_grammy))\n",
    "                                print(f\"{album_id} lost {grammy_id}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"\\nAlbum {index} does not contain songs fromn Top 100\")\n",
    "    else:\n",
    "        print(f\"\\nNo tracks found for album {index}\")\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'albums.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
