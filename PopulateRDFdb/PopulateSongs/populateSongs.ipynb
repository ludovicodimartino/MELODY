{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Songs\n",
    "\n",
    "This notebook is for populating the RDF database of Songs in billboard starting from the following __.csv__ files:\n",
    "- [songs.csv](../../csv/musicoset_metadata/songs.csv)\n",
    "- [song_chart.csv](../../csv/musicoset_popularity/song_chart.csv)\n",
    "- [acoustic_features.csv](../../csv/musicoset_songfeatures/acoustic_features.csv)\n",
    "- [the_grammy_awards_mapped.csv](../../csv/the_grammy_awards_mapped.csv)\n",
    "\n",
    "In order to match for the entries in the different datasets, we used the __fuzzywuzzy__ package for string matching.\n",
    "\n",
    "Before running the code you need to install the following packages:\n",
    "- <code>pip install fuzzywuzzy[speedup]</code>\n",
    "- <code>pip install pandas</code>\n",
    "- <code>pip install rdflib</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import hashlib\n",
    "import re\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import XSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path of the songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.parent.absolute())\n",
    "songsUrl = path + '/csv/musicoset_metadata/songs.csv'\n",
    "songInChartUrl = path + '/csv/musicoset_popularity/song_chart.csv'\n",
    "acousticFeaturesUrl = path + '/csv/musicoset_songfeatures/acoustic_features.csv'\n",
    "grammyUrl = path + '/csv/the_grammy_awards_mapped_uppercase.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/PopulateRDFdb/PopulateSongs/'\n",
    "\n",
    "# print the paths\n",
    "print(f'''\n",
    "    Songs path: {songsUrl}\n",
    "    Chart path: {songInChartUrl}\n",
    "    Acoustic features path: {acousticFeaturesUrl}\n",
    "    Grammy path: {grammyUrl}\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "songs = pd.read_csv(songsUrl, sep='\\t', index_col='song_id')\n",
    "acousticFeatures = pd.read_csv(acousticFeaturesUrl, sep='\\t', index_col='song_id')\n",
    "songCharts = pd.read_csv(songInChartUrl, sep='\\t', index_col='song_id')\n",
    "grammy = pd.read_csv(grammyUrl, sep=',', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "songs.info()\n",
    "acousticFeatures.info()\n",
    "songCharts.info()\n",
    "grammy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namespaces and binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the melody ontology namespace not known by RDFlib\n",
    "MEL = Namespace(\"http://www.dei.unipd.it/~gdb/ontology/melody#\")\n",
    "\n",
    "# Create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"mel\", MEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique id functions\n",
    "\n",
    "These functions are used to create unique IDs of the GRAMMYs and the charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammy_id(year, category):\n",
    "    \n",
    "    year = str(year)\n",
    "    category = str(category) if pd.notna(category) else ''\n",
    "\n",
    "    # Pulizia e normalizzazione dei dati\n",
    "    def clean_text(text):\n",
    "        # Rimuove caratteri speciali e converte in lowercase\n",
    "        return re.sub(r'[^\\w\\s-]', '', text).lower().strip()\n",
    "    \n",
    "    # Crea una stringa concatenata con tutti i dati\n",
    "    full_string = f\"{year}_{clean_text(category)}\"\n",
    "    \n",
    "    # Genera un hash SHA-256 troncato\n",
    "    hash_object = hashlib.sha256(full_string.encode())\n",
    "    short_hash = hash_object.hexdigest()[:8]\n",
    "    \n",
    "    # Crea l'ID finale\n",
    "    category_abbr = ''.join(word[0] for word in clean_text(category).split()[:3])\n",
    "    final_id = f\"{year}_{category_abbr}_{short_hash}\"\n",
    "    \n",
    "    return final_id\n",
    "\n",
    "\n",
    "def create_chart_id(date):\n",
    "    return \"bil_\" + str(date).replace(\"-\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songs to remove from the RDF dataset\n",
    "There are songs that are present in the [songs.csv](../../csv/musicoset_metadata/songs.csv) file but not in the [song_chart.csv](../../csv/musicoset_popularity/song_chart.csv) file. These songs should not be mapped in the RDF dataset because are not of interest to us. We only consider songs that have been at least once in a Billboard Hot 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songsIDToRemove = songs[~songs.index.isin(songCharts.index)].index\n",
    "print(songsIDToRemove.to_list())\n",
    "songs = songs[~songs.index.isin(songsIDToRemove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remix matching\n",
    "Using the titles and artist names, identify whether a remix has an original version in the Billboard Hot 100 song dataset.\n",
    "The potential original songs are those that include at least one artist featured in the remix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- remix matching ---\")\n",
    "\n",
    "found = 0 # The number of found associations\n",
    "searchThreshold = 90\n",
    "for index, row in songs.iterrows():\n",
    "    # Consider only the songs that contains the word \"remix\" in the title\n",
    "    if \"remix\" in row[\"song_name\"].lower():\n",
    "        # Extract the artist/s of the song to get a list of possible original songs\n",
    "        possibleOriginalSongs = []\n",
    "        for index2, row2 in songs.iterrows():\n",
    "            # Skip the song with the same id\n",
    "            if index2 == index:\n",
    "                continue\n",
    "            if any(id in eval(row[\"artists\"]).keys() for id in eval(row2[\"artists\"]).keys()):\n",
    "                possibleOriginalSongs.append(row2[\"song_name\"])\n",
    "\n",
    "        # If there are some possible original song try to find the one that matches the title\n",
    "        if len(possibleOriginalSongs) > 0:\n",
    "            result = process.extractOne(row['song_name'], songs[songs['song_name'].isin(possibleOriginalSongs)]['song_name'])\n",
    "            if result[1] >= searchThreshold: # original song found\n",
    "                print(\"Adding to the graph: \" + row['song_name'] + \" --> \" + result[0])\n",
    "                # Add the triple in the graph\n",
    "                g.add((URIRef(MEL[index]), MEL['isRemixOf'], URIRef(MEL[result[2]])))\n",
    "                found += 1\n",
    "\n",
    "print(\"Number of REMIX/song associations found: \", found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the songs to the GRAMMYs\n",
    "\n",
    "The mapping is based on the **title of the songs**. Some cleaning is necessary before the actual matching that is performed using the *FuzzyWuzzy* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- song to grammy mapping ---\")\n",
    "\n",
    "found = 0 # Count the number of found associations\n",
    "searchThreshold = 98\n",
    "\n",
    "# clean the titles of the songs to make these match to the grammy nominee field\n",
    "words_to_remove = ['version', 'theme', 'single', 'mono'] # list of words that causes noise in the titles\n",
    "pattern = r'\\b(?:' + '|'.join(words_to_remove) + r')\\b'\n",
    "cleanSongsTitlesDf = songs['song_name'].str.lower() # All the titles in lowercase for a better matching\n",
    "cleanSongsTitlesDf = cleanSongsTitlesDf.str.replace(pattern, '', regex=True).str.strip() # remove noisy words\n",
    "cleanSongsTitlesDf = cleanSongsTitlesDf.str.replace(r'-.*$', '', regex=True).str.strip() # remove all what comes after an \"-\"\n",
    "\n",
    "# Add song-grammy mapping\n",
    "for grammyIndex, grammyRow in grammy.iterrows():\n",
    "    \n",
    "    # skip if the grammy category contains the word album or artist (we want to map just the songs)\n",
    "    # and avoid all the categories without a nominee (it means it is a grammy for an artist)\n",
    "    if all(keyword not in grammyRow['category'].lower() for keyword in [\"album\", \"artist\"]) and grammyRow['workers']:\n",
    "        \n",
    "        # clean the nominee field for a better match\n",
    "        nominee = grammyRow['nominee'].lower() # nominee string to lowercase\n",
    "        nominee = re.sub(pattern, '', nominee) # Remove noisy words\n",
    "        nominee = nominee.split('.')[0] # Clear what comes after the \".\"\n",
    "\n",
    "        # Try to find the associated song in the song dataset\n",
    "        result = process.extractOne(nominee, cleanSongsTitlesDf, scorer=fuzz.ratio)\n",
    "        \n",
    "        # If it is a match according to the search threshold\n",
    "        if result[1] >= searchThreshold:\n",
    "            found += 1\n",
    "            # Add the triple to the graph\n",
    "            Grammy = URIRef(MEL[create_grammy_id(\n",
    "                grammyRow['year'], \n",
    "                grammyRow['category'])])\n",
    "            \n",
    "            if grammyRow['winner']:\n",
    "                g.add((URIRef(MEL[result[2]]), MEL['winner'], Grammy))\n",
    "            else:\n",
    "                g.add((URIRef(MEL[result[2]]), MEL['candidated'], Grammy))\n",
    "            \n",
    "            print(\"Adding to the graph: \" + result[0] + \" --> \" + nominee + \" \" + str(result[1]))\n",
    "            \n",
    "print(\"Found associations between grammy and songs: \", found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song feature mapping\n",
    "The feature of the songs are added to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- song features mapping ---\")\n",
    "\n",
    "# keyMap array used to parse the key of the song\n",
    "keyMap = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "\n",
    "for index, row in acousticFeatures.iterrows():\n",
    "    # Check if the index is present in the songs pandas dataframe.\n",
    "    if (songs.index == index).any():\n",
    "        Song = URIRef(MEL[index])\n",
    "        songMetadata = songs[songs.index == index]\n",
    "        g.add((Song, RDF.type, MEL.Song))\n",
    "        g.add((Song, MEL['acousticness'], Literal(row['acousticness'], datatype=XSD.float)))\n",
    "        \n",
    "        bpm = round(row['tempo'])\n",
    "        if bpm > 0:\n",
    "            g.add((Song, MEL['bpm'], Literal(bpm, datatype=XSD.positiveInteger)))\n",
    "        \n",
    "        g.add((Song, MEL['danceability'], Literal(row['danceability'], datatype=XSD.float)))\n",
    "        g.add((Song, MEL['valence'], Literal(row['valence'], datatype=XSD.float)))\n",
    "        \n",
    "        duration = int(row['duration_ms'])\n",
    "        if duration > 0:\n",
    "            g.add((Song, MEL['duration'], Literal(duration, datatype=XSD.positiveInteger)))\n",
    "        \n",
    "        # Add key and mode if present\n",
    "        if int(row['key']) != -1 and row['mode'] in {0, 1}:\n",
    "            g.add((Song, MEL['key'], Literal((keyMap[int(row['key'])] + (\"m\" if row['mode'] == 0 else \"\")), lang=\"en\")))\n",
    "        \n",
    "        # Add song name\n",
    "        songName = str(songMetadata['song_name'].iloc[0])\n",
    "        g.add((Song, MEL['name'], Literal(songName, datatype=XSD.string)))\n",
    "\n",
    "        # Add song type and check if it is compliant\n",
    "        if(songMetadata['song_type'].iloc[0] in {\"Collaboration\", \"Solo\"}):\n",
    "            g.add((Song, MEL['songType'], Literal(songMetadata['song_type'].iloc[0], lang=\"en\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song to artist mapping\n",
    "\n",
    "Each song is linked to one or more artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- song to artist mapping ---\")\n",
    "\n",
    "for index, row in songs.iterrows():\n",
    "    Song = URIRef(MEL[index])\n",
    "    # Add artist/s\n",
    "    for artist_id in eval(row[\"artists\"]).keys():\n",
    "        g.add((Song, MEL['sungBy'], URIRef(MEL[artist_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song to Billboard mapping\n",
    "The songs in the [songs.csv](../../csv/musicoset_metadata/songs.csv) file are taken form the Billboard Hot 100 from the last 56 years, therefore we mapped each song to its Billboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- song to billboard mapping ---\")\n",
    "\n",
    "for index, row in songCharts.iterrows():\n",
    "    Song = URIRef(MEL[index])\n",
    "    Membership = URIRef(MEL[\"m_\" + index + \"_\" + str(row['rank_score'])])\n",
    "    Billboard = URIRef(MEL[create_chart_id(row['week'])])\n",
    "\n",
    "    g.add((Membership, RDF.type, MEL.Membership))\n",
    "    g.add((Billboard, RDF.type, MEL.BillboardHot100))\n",
    "    g.add((Song, MEL['classified'], Membership))\n",
    "    g.add((Membership, MEL['position'], Literal(row['rank_score'], datatype=XSD.positiveInteger)))\n",
    "    g.add((Membership, MEL['classifiedIn'], Billboard))\n",
    "    g.add((Billboard, MEL['week'], Literal(row['week'], datatype=XSD.date)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the turtle serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- saving serialization ---\")\n",
    "\n",
    "with open(savePath + 'songs.ttl', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
